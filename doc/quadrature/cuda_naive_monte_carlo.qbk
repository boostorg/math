[/
Copyright (c) 2018 Nick Thompson
Use, modification and distribution are subject to the
Boost Software License, Version 1.0. (See accompanying file
LICENSE_1_0.txt or copy at http://www.boost.org/LICENSE_1_0.txt)
]


[section:cuda_naive_monte_carlo CUDA Accelerated Naive Monte Carlo Integration]

[heading Synopsis]

    #include <boost/math/quadrature/cuda_naive_monte_carlo.hpp>
    namespace boost { namespace math { namespace quadrature {

    template <class Real, class F, class ThreadGen = thrust::random::taus88, class MasterGen = std::random_device>
    struct cuda_naive_monte_carlo
    {
    public:
        cuda_naive_monte_carlo(
		   const F& integrand,
           std::vector<std::pair<Real, Real>> const & bounds, 
		   const MasterGen& seed);
		cuda_naive_monte_carlo(
		   const F& integrand, 
		   std::vector<std::pair<Real, Real>> const & bounds)

        Real integrate(
		   Real error_request, 
		   boost::uintmax_t calls_per_thread = 1024, 
		   boost::uintmax_t max_calls_per_thread = 250000, 
		   bool is_compensated = true);

      Real variance() const;
      Real current_error_estimate() const;
      uint64_t calls() const;
    };
    }}} // namespaces

[heading Description]

The class `cuda_naive_monte_carlo` performs Monte-Carlo integration on a square integrable function /f/ on a domain [Omega].
The theoretical background of Monte-Carlo integration is nicely discussed at [@https://en.wikipedia.org/wiki/Monte_Carlo_integration Wikipedia],
and as such will not be discussed here.
However, despite being "naive",
it is a mistake to assume that naive Monte-Carlo integration is not powerful,
as the simplicity of the method affords a robustness not easily provided by more sophisticated tools.
The multithreaded nature of the routine allows us to compute a large number of sample points with great speed,
and hence the slow convergence is mitigated by exploiting the full power of modern hardware.

This class provides proof-of-principle CUDA-accelerated Monte-Carlo integration which will utilize all available
CUDA threads to provide a significant performance advantage over non-CUDA code.  Since CUDA is used, only
types `float` and `double` are supported.  Note that there are two random number generators specified in the
template type-list: type `ThreadGen` is the per-thread random number generator, and must be a type which is
usable on the CUDA device.  These per-thread generators are initialized via random seeds generated by an object
of type `MasterGen`: this defaults to `std::random_device` but could equally be a psuedo-random number generator.
It is important though to ensure that the per-thread random generators do not become correlated, or generate
overlapping sequences.  For this reason type `ThreadGen` should have a long repeat period, and at the very least 
be a different type from `MasterGen`.

A call to member function `integrate` performs the actual integration, and also controls how the CUDA device is used:

        Real integrate(
		   Real error_request, 
		   boost::uintmax_t calls_per_thread = 1024, 
		   boost::uintmax_t max_calls_per_thread = 250000, 
		   bool is_compensated = true);

The parameters are as follows:

* `error_request`: the desired absolute error in the result.
* `calls_per_thread`: the number of calls to the integrand made by each thread in the first pass - 
this first pass is used to get an estimate of the variance and calculate how many calls will
be required by the second pass to reach the desired error bound.
* `max_calls_per_thread`: the maximum number of calls of the integrand by each thread in any single CUDA
invocation.  This parameter is used to prevent operating system timeouts from terminating the application.
For example on Windows if the CUDA accelerated display driver is unresponsive for more than 2 seconds, then
the application using it will simply be terminated.
* `is_compensated`: when `true` each thread will use Kahan-style compensated addition to ensure accuracy.  When
`false` then regular addition will be used for improved performance.

For example:

      // Define a function to integrate:
      auto g = [] __device__ (const double* x)
      {
         constexpr const double pi = boost::math::constants::pi<double>();
         constexpr const double A = 1.0 / (pi * pi * pi);
         return A / (1.0 - cos(x[0])*cos(x[1])*cos(x[2]));
      };
      std::vector<std::pair<double, double>> bounds{ { 0, boost::math::constants::pi<double>() },{ 0, boost::math::constants::pi<double>() },{ 0, boost::math::constants::pi<double>() } };
      double error_goal = 0.001;
      cuda_naive_monte_carlo<double, decltype(g)> mc(g, bounds);

      double result = mc.integrate(error_goal);

      std::cout << "Integration result is: " << result << std::endl;

First off, we define the function we wish to integrate.
This function must accept a `const Real*`, and return a `Real`.
In comparison to the regular non-CUDA code we loose a good deal of type safety
as CUDA deals in raw pointers, not vectors - so it's up to the client to
ensure that the number of elements accessed matches the number of dimensions passed
to the integrators constructor.
Also note that we've used a lambda expression for the integrand: this currently requires
compilation with `-expt-extended-lambda`.

Next, we define the domain of integration as a vector of pairs - in this case
the domain is [0, PI] over 3 dimensions.

   std::vector<std::pair<double, double>> bounds{ { 0, boost::math::constants::pi<double>() },{ 0, boost::math::constants::pi<double>() },{ 0, boost::math::constants::pi<double>() } };

The call

    naive_monte_carlo<double, decltype(g)> mc(g, bounds);

creates an instance of the Monte-Carlo integrator, and the call to `integrate` then returns the result.

[endsect]
